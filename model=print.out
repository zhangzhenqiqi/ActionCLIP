Loading cudnn7.6-cuda10.2/7.6.5.32
  Loading requirement: cuda10.2/toolkit/10.2.89
Loading nccl2-cuda10.2-gcc/2.6.4
  Loading requirement: gcc5/5.5.0
ActionClip boot~
<string>:1: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
<string>:1: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
<string>:1: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/home/10501001/projects/ActionCLIP/train.py:53: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(f)
wandb: Currently logged in as: wozzq (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.12.4
wandb: Syncing run 20211016_202558_clip_ucf_ViT-B/16_ucf101
wandb:  View project at https://wandb.ai/wozzq/clip_ucf
wandb:  View run at https://wandb.ai/wozzq/clip_ucf/runs/3dsu0bw6
wandb: Run data is saved locally in /home/10501001/projects/ActionCLIP/wandb/run-20211016_202644-3dsu0bw6
wandb: Run `wandb offline` to turn off syncing.

--------------------------------------------------------------------------------
                     working dir: ./exp/clip_ucf/ViT-B/16/ucf101/20211016_202558
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
                               Config
{   'data': {   'batch_size': 32,
                'dataset': 'ucf101',
                'gpus': 1,
                'image_tmpl': 'img_{:05d}.jpg',
                'index_bias': 1,
                'input_size': 224,
                'label_list': 'lists/ucf_labels.csv',
                'modality': 'RGB',
                'num_classes': 101,
                'num_segments': 8,
                'randaug': {'M': 0, 'N': 0},
                'seg_length': 1,
                'split': 1,
                'train_list': '/home/10501001/datasets/ucf101/ucfTrainTestlist/train_rgb_split1.txt',
                'val_list': '/home/10501001/datasets/ucf101/ucfTrainTestlist/val_rgb_split1.txt',
                'workers': 8},
    'logging': {'eval_freq': 1, 'print_freq': 10},
    'network': {   'arch': 'ViT-B/16',
                   'describe': None,
                   'drop_out': 0.0,
                   'emb_dropout': 0.0,
                   'fix_img': False,
                   'fix_text': False,
                   'init': True,
                   'sim_header': 'Transf',
                   'type': 'clip_ucf'},
    'pretrain': '/home/10501001/models/vit-16.pt',
    'resume': None,
    'seed': 1024,
    'solver': {   'clip_gradient': 20,
                  'epoch_offset': 0,
                  'epochs': 50,
                  'evaluate': False,
                  'f_ratio': 10,
                  'loss_type': 'nll',
                  'lr': 5e-06,
                  'lr_decay_factor': 0.1,
                  'lr_decay_step': 15,
                  'lr_warmup_step': 5,
                  'momentum': 0.9,
                  'optim': 'adamw',
                  'ratio': 1,
                  'start_epoch': 0,
                  'type': 'cosine',
                  'weight_decay': 0.2}}
--------------------------------------------------------------------------------
dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
loading clip pretrained model!
train transforms: [Compose(
    <datasets.transforms_ss.GroupMultiScaleCrop object at 0x2aab4a1735b0>
    <datasets.transforms_ss.GroupRandomHorizontalFlip object at 0x2aab4a173f40>
    <datasets.transforms_ss.GroupRandomColorJitter object at 0x2aab4a173f70>
    <datasets.transforms_ss.GroupRandomGrayscale object at 0x2aab4a173ee0>
    <datasets.transforms_ss.GroupGaussianBlur object at 0x2aab4a173d90>
    <datasets.transforms_ss.GroupSolarization object at 0x2aab4a173d30>
), Compose(
    <datasets.transforms_ss.Stack object at 0x2aab4a173c40>
    <datasets.transforms_ss.ToTorchFormatTensor object at 0x2aab4a173be0>
    <datasets.transforms_ss.GroupNormalize object at 0x2aab4a173a60>
)]
val transforms: [Compose(
    <datasets.transforms_ss.GroupScale object at 0x2aab4a173ac0>
    <datasets.transforms_ss.GroupCenterCrop object at 0x2aab4a173e50>
), Compose(
    <datasets.transforms_ss.Stack object at 0x2aab4a1738e0>
    <datasets.transforms_ss.ToTorchFormatTensor object at 0x2aab4a173610>
    <datasets.transforms_ss.GroupNormalize object at 0x2aab4a173880>
)]
layer=6
model=model=model=model=model=model=model=model=model=model=model=model=model=model=model=model=model=model=model=model=
CLIP(
  (visual): VisualTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
)
fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=fusion_model=
DataParallel(
  (module): visual_prompt(
    (frame_position_embeddings): Embedding(77, 512)
    (transformer): TemporalTransformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm()
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm()
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm()
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm()
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm()
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm()
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm()
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm()
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm()
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm()
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm()
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm()
        )
      )
    )
  )
)
random_shift:DotMap()
random_shift:DotMap()
=========using KL Loss=and has temperature and * bz==========
=========using KL Loss=and has temperature and * bz==========
=> loading checkpoint '/home/10501001/models/vit-16.pt'
5e-06
5e-06
5e-05
AdamW
positional_embedding: True
text_projection: True
logit_scale: True
visual.class_embedding: True
visual.positional_embedding: True
visual.proj: True
visual.conv1.weight: True
visual.ln_pre.weight: True
visual.ln_pre.bias: True
visual.transformer.resblocks.0.attn.in_proj_weight: True
visual.transformer.resblocks.0.attn.in_proj_bias: True
visual.transformer.resblocks.0.attn.out_proj.weight: True
visual.transformer.resblocks.0.attn.out_proj.bias: True
visual.transformer.resblocks.0.ln_1.weight: True
visual.transformer.resblocks.0.ln_1.bias: True
visual.transformer.resblocks.0.mlp.c_fc.weight: True
visual.transformer.resblocks.0.mlp.c_fc.bias: True
visual.transformer.resblocks.0.mlp.c_proj.weight: True
visual.transformer.resblocks.0.mlp.c_proj.bias: True
visual.transformer.resblocks.0.ln_2.weight: True
visual.transformer.resblocks.0.ln_2.bias: True
visual.transformer.resblocks.1.attn.in_proj_weight: True
visual.transformer.resblocks.1.attn.in_proj_bias: True
visual.transformer.resblocks.1.attn.out_proj.weight: True
visual.transformer.resblocks.1.attn.out_proj.bias: True
visual.transformer.resblocks.1.ln_1.weight: True
visual.transformer.resblocks.1.ln_1.bias: True
visual.transformer.resblocks.1.mlp.c_fc.weight: True
visual.transformer.resblocks.1.mlp.c_fc.bias: True
visual.transformer.resblocks.1.mlp.c_proj.weight: True
visual.transformer.resblocks.1.mlp.c_proj.bias: True
visual.transformer.resblocks.1.ln_2.weight: True
visual.transformer.resblocks.1.ln_2.bias: True
visual.transformer.resblocks.2.attn.in_proj_weight: True
visual.transformer.resblocks.2.attn.in_proj_bias: True
visual.transformer.resblocks.2.attn.out_proj.weight: True
visual.transformer.resblocks.2.attn.out_proj.bias: True
visual.transformer.resblocks.2.ln_1.weight: True
visual.transformer.resblocks.2.ln_1.bias: True
visual.transformer.resblocks.2.mlp.c_fc.weight: True
visual.transformer.resblocks.2.mlp.c_fc.bias: True
visual.transformer.resblocks.2.mlp.c_proj.weight: True
visual.transformer.resblocks.2.mlp.c_proj.bias: True
visual.transformer.resblocks.2.ln_2.weight: True
visual.transformer.resblocks.2.ln_2.bias: True
visual.transformer.resblocks.3.attn.in_proj_weight: True
visual.transformer.resblocks.3.attn.in_proj_bias: True
visual.transformer.resblocks.3.attn.out_proj.weight: True
visual.transformer.resblocks.3.attn.out_proj.bias: True
visual.transformer.resblocks.3.ln_1.weight: True
visual.transformer.resblocks.3.ln_1.bias: True
visual.transformer.resblocks.3.mlp.c_fc.weight: True
visual.transformer.resblocks.3.mlp.c_fc.bias: True
visual.transformer.resblocks.3.mlp.c_proj.weight: True
visual.transformer.resblocks.3.mlp.c_proj.bias: True
visual.transformer.resblocks.3.ln_2.weight: True
visual.transformer.resblocks.3.ln_2.bias: True
visual.transformer.resblocks.4.attn.in_proj_weight: True
visual.transformer.resblocks.4.attn.in_proj_bias: True
visual.transformer.resblocks.4.attn.out_proj.weight: True
visual.transformer.resblocks.4.attn.out_proj.bias: True
visual.transformer.resblocks.4.ln_1.weight: True
visual.transformer.resblocks.4.ln_1.bias: True
visual.transformer.resblocks.4.mlp.c_fc.weight: True
visual.transformer.resblocks.4.mlp.c_fc.bias: True
visual.transformer.resblocks.4.mlp.c_proj.weight: True
visual.transformer.resblocks.4.mlp.c_proj.bias: True
visual.transformer.resblocks.4.ln_2.weight: True
visual.transformer.resblocks.4.ln_2.bias: True
visual.transformer.resblocks.5.attn.in_proj_weight: True
visual.transformer.resblocks.5.attn.in_proj_bias: True
visual.transformer.resblocks.5.attn.out_proj.weight: True
visual.transformer.resblocks.5.attn.out_proj.bias: True
visual.transformer.resblocks.5.ln_1.weight: True
visual.transformer.resblocks.5.ln_1.bias: True
visual.transformer.resblocks.5.mlp.c_fc.weight: True
visual.transformer.resblocks.5.mlp.c_fc.bias: True
visual.transformer.resblocks.5.mlp.c_proj.weight: True
visual.transformer.resblocks.5.mlp.c_proj.bias: True
visual.transformer.resblocks.5.ln_2.weight: True
visual.transformer.resblocks.5.ln_2.bias: True
visual.transformer.resblocks.6.attn.in_proj_weight: True
visual.transformer.resblocks.6.attn.in_proj_bias: True
visual.transformer.resblocks.6.attn.out_proj.weight: True
visual.transformer.resblocks.6.attn.out_proj.bias: True
visual.transformer.resblocks.6.ln_1.weight: True
visual.transformer.resblocks.6.ln_1.bias: True
visual.transformer.resblocks.6.mlp.c_fc.weight: True
visual.transformer.resblocks.6.mlp.c_fc.bias: True
visual.transformer.resblocks.6.mlp.c_proj.weight: True
visual.transformer.resblocks.6.mlp.c_proj.bias: True
visual.transformer.resblocks.6.ln_2.weight: True
visual.transformer.resblocks.6.ln_2.bias: True
visual.transformer.resblocks.7.attn.in_proj_weight: True
visual.transformer.resblocks.7.attn.in_proj_bias: True
visual.transformer.resblocks.7.attn.out_proj.weight: True
visual.transformer.resblocks.7.attn.out_proj.bias: True
visual.transformer.resblocks.7.ln_1.weight: True
visual.transformer.resblocks.7.ln_1.bias: True
visual.transformer.resblocks.7.mlp.c_fc.weight: True
visual.transformer.resblocks.7.mlp.c_fc.bias: True
visual.transformer.resblocks.7.mlp.c_proj.weight: True
visual.transformer.resblocks.7.mlp.c_proj.bias: True
visual.transformer.resblocks.7.ln_2.weight: True
visual.transformer.resblocks.7.ln_2.bias: True
visual.transformer.resblocks.8.attn.in_proj_weight: True
visual.transformer.resblocks.8.attn.in_proj_bias: True
visual.transformer.resblocks.8.attn.out_proj.weight: True
visual.transformer.resblocks.8.attn.out_proj.bias: True
visual.transformer.resblocks.8.ln_1.weight: True
visual.transformer.resblocks.8.ln_1.bias: True
visual.transformer.resblocks.8.mlp.c_fc.weight: True
visual.transformer.resblocks.8.mlp.c_fc.bias: True
visual.transformer.resblocks.8.mlp.c_proj.weight: True
visual.transformer.resblocks.8.mlp.c_proj.bias: True
visual.transformer.resblocks.8.ln_2.weight: True
visual.transformer.resblocks.8.ln_2.bias: True
visual.transformer.resblocks.9.attn.in_proj_weight: True
visual.transformer.resblocks.9.attn.in_proj_bias: True
visual.transformer.resblocks.9.attn.out_proj.weight: True
visual.transformer.resblocks.9.attn.out_proj.bias: True
visual.transformer.resblocks.9.ln_1.weight: True
visual.transformer.resblocks.9.ln_1.bias: True
visual.transformer.resblocks.9.mlp.c_fc.weight: True
visual.transformer.resblocks.9.mlp.c_fc.bias: True
visual.transformer.resblocks.9.mlp.c_proj.weight: True
visual.transformer.resblocks.9.mlp.c_proj.bias: True
visual.transformer.resblocks.9.ln_2.weight: True
visual.transformer.resblocks.9.ln_2.bias: True
visual.transformer.resblocks.10.attn.in_proj_weight: True
visual.transformer.resblocks.10.attn.in_proj_bias: True
visual.transformer.resblocks.10.attn.out_proj.weight: True
visual.transformer.resblocks.10.attn.out_proj.bias: True
visual.transformer.resblocks.10.ln_1.weight: True
visual.transformer.resblocks.10.ln_1.bias: True
visual.transformer.resblocks.10.mlp.c_fc.weight: True
visual.transformer.resblocks.10.mlp.c_fc.bias: True
visual.transformer.resblocks.10.mlp.c_proj.weight: True
visual.transformer.resblocks.10.mlp.c_proj.bias: True
visual.transformer.resblocks.10.ln_2.weight: True
visual.transformer.resblocks.10.ln_2.bias: True
visual.transformer.resblocks.11.attn.in_proj_weight: True
visual.transformer.resblocks.11.attn.in_proj_bias: True
visual.transformer.resblocks.11.attn.out_proj.weight: True
visual.transformer.resblocks.11.attn.out_proj.bias: True
visual.transformer.resblocks.11.ln_1.weight: True
visual.transformer.resblocks.11.ln_1.bias: True
visual.transformer.resblocks.11.mlp.c_fc.weight: True
visual.transformer.resblocks.11.mlp.c_fc.bias: True
visual.transformer.resblocks.11.mlp.c_proj.weight: True
visual.transformer.resblocks.11.mlp.c_proj.bias: True
visual.transformer.resblocks.11.ln_2.weight: True
visual.transformer.resblocks.11.ln_2.bias: True
visual.ln_post.weight: True
visual.ln_post.bias: True
transformer.resblocks.0.attn.in_proj_weight: True
transformer.resblocks.0.attn.in_proj_bias: True
transformer.resblocks.0.attn.out_proj.weight: True
transformer.resblocks.0.attn.out_proj.bias: True
transformer.resblocks.0.ln_1.weight: True
transformer.resblocks.0.ln_1.bias: True
transformer.resblocks.0.mlp.c_fc.weight: True
transformer.resblocks.0.mlp.c_fc.bias: True
transformer.resblocks.0.mlp.c_proj.weight: True
transformer.resblocks.0.mlp.c_proj.bias: True
transformer.resblocks.0.ln_2.weight: True
transformer.resblocks.0.ln_2.bias: True
transformer.resblocks.1.attn.in_proj_weight: True
transformer.resblocks.1.attn.in_proj_bias: True
transformer.resblocks.1.attn.out_proj.weight: True
transformer.resblocks.1.attn.out_proj.bias: True
transformer.resblocks.1.ln_1.weight: True
transformer.resblocks.1.ln_1.bias: True
transformer.resblocks.1.mlp.c_fc.weight: True
transformer.resblocks.1.mlp.c_fc.bias: True
transformer.resblocks.1.mlp.c_proj.weight: True
transformer.resblocks.1.mlp.c_proj.bias: True
transformer.resblocks.1.ln_2.weight: True
transformer.resblocks.1.ln_2.bias: True
transformer.resblocks.2.attn.in_proj_weight: True
transformer.resblocks.2.attn.in_proj_bias: True
transformer.resblocks.2.attn.out_proj.weight: True
transformer.resblocks.2.attn.out_proj.bias: True
transformer.resblocks.2.ln_1.weight: True
transformer.resblocks.2.ln_1.bias: True
transformer.resblocks.2.mlp.c_fc.weight: True
transformer.resblocks.2.mlp.c_fc.bias: True
transformer.resblocks.2.mlp.c_proj.weight: True
transformer.resblocks.2.mlp.c_proj.bias: True
transformer.resblocks.2.ln_2.weight: True
transformer.resblocks.2.ln_2.bias: True
transformer.resblocks.3.attn.in_proj_weight: True
transformer.resblocks.3.attn.in_proj_bias: True
transformer.resblocks.3.attn.out_proj.weight: True
transformer.resblocks.3.attn.out_proj.bias: True
transformer.resblocks.3.ln_1.weight: True
transformer.resblocks.3.ln_1.bias: True
transformer.resblocks.3.mlp.c_fc.weight: True
transformer.resblocks.3.mlp.c_fc.bias: True
transformer.resblocks.3.mlp.c_proj.weight: True
transformer.resblocks.3.mlp.c_proj.bias: True
transformer.resblocks.3.ln_2.weight: True
transformer.resblocks.3.ln_2.bias: True
transformer.resblocks.4.attn.in_proj_weight: True
transformer.resblocks.4.attn.in_proj_bias: True
transformer.resblocks.4.attn.out_proj.weight: True
transformer.resblocks.4.attn.out_proj.bias: True
transformer.resblocks.4.ln_1.weight: True
transformer.resblocks.4.ln_1.bias: True
transformer.resblocks.4.mlp.c_fc.weight: True
transformer.resblocks.4.mlp.c_fc.bias: True
transformer.resblocks.4.mlp.c_proj.weight: True
transformer.resblocks.4.mlp.c_proj.bias: True
transformer.resblocks.4.ln_2.weight: True
transformer.resblocks.4.ln_2.bias: True
transformer.resblocks.5.attn.in_proj_weight: True
transformer.resblocks.5.attn.in_proj_bias: True
transformer.resblocks.5.attn.out_proj.weight: True
transformer.resblocks.5.attn.out_proj.bias: True
transformer.resblocks.5.ln_1.weight: True
transformer.resblocks.5.ln_1.bias: True
transformer.resblocks.5.mlp.c_fc.weight: True
transformer.resblocks.5.mlp.c_fc.bias: True
transformer.resblocks.5.mlp.c_proj.weight: True
transformer.resblocks.5.mlp.c_proj.bias: True
transformer.resblocks.5.ln_2.weight: True
transformer.resblocks.5.ln_2.bias: True
transformer.resblocks.6.attn.in_proj_weight: True
transformer.resblocks.6.attn.in_proj_bias: True
transformer.resblocks.6.attn.out_proj.weight: True
transformer.resblocks.6.attn.out_proj.bias: True
transformer.resblocks.6.ln_1.weight: True
transformer.resblocks.6.ln_1.bias: True
transformer.resblocks.6.mlp.c_fc.weight: True
transformer.resblocks.6.mlp.c_fc.bias: True
transformer.resblocks.6.mlp.c_proj.weight: True
transformer.resblocks.6.mlp.c_proj.bias: True
transformer.resblocks.6.ln_2.weight: True
transformer.resblocks.6.ln_2.bias: True
transformer.resblocks.7.attn.in_proj_weight: True
transformer.resblocks.7.attn.in_proj_bias: True
transformer.resblocks.7.attn.out_proj.weight: True
transformer.resblocks.7.attn.out_proj.bias: True
transformer.resblocks.7.ln_1.weight: True
transformer.resblocks.7.ln_1.bias: True
transformer.resblocks.7.mlp.c_fc.weight: True
transformer.resblocks.7.mlp.c_fc.bias: True
transformer.resblocks.7.mlp.c_proj.weight: True
transformer.resblocks.7.mlp.c_proj.bias: True
transformer.resblocks.7.ln_2.weight: True
transformer.resblocks.7.ln_2.bias: True
transformer.resblocks.8.attn.in_proj_weight: True
transformer.resblocks.8.attn.in_proj_bias: True
transformer.resblocks.8.attn.out_proj.weight: True
transformer.resblocks.8.attn.out_proj.bias: True
transformer.resblocks.8.ln_1.weight: True
transformer.resblocks.8.ln_1.bias: True
transformer.resblocks.8.mlp.c_fc.weight: True
transformer.resblocks.8.mlp.c_fc.bias: True
transformer.resblocks.8.mlp.c_proj.weight: True
transformer.resblocks.8.mlp.c_proj.bias: True
transformer.resblocks.8.ln_2.weight: True
transformer.resblocks.8.ln_2.bias: True
transformer.resblocks.9.attn.in_proj_weight: True
transformer.resblocks.9.attn.in_proj_bias: True
transformer.resblocks.9.attn.out_proj.weight: True
transformer.resblocks.9.attn.out_proj.bias: True
transformer.resblocks.9.ln_1.weight: True
transformer.resblocks.9.ln_1.bias: True
transformer.resblocks.9.mlp.c_fc.weight: True
transformer.resblocks.9.mlp.c_fc.bias: True
transformer.resblocks.9.mlp.c_proj.weight: True
transformer.resblocks.9.mlp.c_proj.bias: True
transformer.resblocks.9.ln_2.weight: True
transformer.resblocks.9.ln_2.bias: True
transformer.resblocks.10.attn.in_proj_weight: True
transformer.resblocks.10.attn.in_proj_bias: True
transformer.resblocks.10.attn.out_proj.weight: True
transformer.resblocks.10.attn.out_proj.bias: True
transformer.resblocks.10.ln_1.weight: True
transformer.resblocks.10.ln_1.bias: True
transformer.resblocks.10.mlp.c_fc.weight: True
transformer.resblocks.10.mlp.c_fc.bias: True
transformer.resblocks.10.mlp.c_proj.weight: True
transformer.resblocks.10.mlp.c_proj.bias: True
transformer.resblocks.10.ln_2.weight: True
transformer.resblocks.10.ln_2.bias: True
transformer.resblocks.11.attn.in_proj_weight: True
transformer.resblocks.11.attn.in_proj_bias: True
transformer.resblocks.11.attn.out_proj.weight: True
transformer.resblocks.11.attn.out_proj.bias: True
transformer.resblocks.11.ln_1.weight: True
transformer.resblocks.11.ln_1.bias: True
transformer.resblocks.11.mlp.c_fc.weight: True
transformer.resblocks.11.mlp.c_fc.bias: True
transformer.resblocks.11.mlp.c_proj.weight: True
transformer.resblocks.11.mlp.c_proj.bias: True
transformer.resblocks.11.ln_2.weight: True
transformer.resblocks.11.ln_2.bias: True
token_embedding.weight: True
ln_final.weight: True
ln_final.bias: True
  0% 0/298 [00:00<?, ?it/s]/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
  0% 0/298 [00:16<?, ?it/s]
Traceback (most recent call last):
  File "/home/10501001/projects/ActionCLIP/train.py", line 228, in <module>
    main()
  File "/home/10501001/projects/ActionCLIP/train.py", line 184, in main
    image_embedding = model_image(images)
  File "/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/10501001/projects/ActionCLIP/train.py", line 42, in forward
    return self.model.encode_image(image)
  File "/home/10501001/projects/ActionCLIP/clip/model.py", line 246, in encode_image
    return self.visual(image.type(self.dtype))
  File "/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/10501001/projects/ActionCLIP/clip/model.py", line 142, in forward
    x = self.transformer(x)
  File "/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/10501001/projects/ActionCLIP/clip/model.py", line 92, in forward
    return self.resblocks(x)
  File "/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/10501001/projects/ActionCLIP/clip/model.py", line 76, in forward
    x = x + self.drop_path(self.mlp(self.ln_2(x)))
  File "/home/10501001/anaconda3/envs/ACTION-CLIP/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/10501001/projects/ActionCLIP/clip/model.py", line 45, in forward
    ret = super().forward(x.type(torch.float32))
RuntimeError: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.65 GiB total capacity; 22.29 GiB already allocated; 48.00 MiB free; 22.58 GiB reserved in total by PyTorch)

wandb: Waiting for W&B process to finish, PID 206535... (failed 1). Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: / 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: - 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: / 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: - 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced 20211016_202558_clip_ucf_ViT-B/16_ucf101: https://wandb.ai/wozzq/clip_ucf/runs/3dsu0bw6
wandb: Find logs at: ./wandb/run-20211016_202644-3dsu0bw6/logs/debug.log
wandb: 
